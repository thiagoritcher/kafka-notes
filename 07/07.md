# Topics and partitions

## 7.1 Topics

topic is a nonconcrete concept rather than a physical structure

- This chapters covers
- Creation parameters and configuration options
- How partitions exist as log files
- How segments impact data inside partitions
- Testing with EmbeddedKafkaCluster Topic compaction and how data can be
retained

no other details are needed for them to subscribe

Kafka writes the data that makes up a topic in the cluster to logs

partitions are made up of messages that are sent to the topic

At the highest level, this impacts how our consumers get to the data

topic design as a two-step process

first looks at the events we have

Do they belong in one topic or more than one?

What is the number of partitions we should use?

partitions are a per-topic design question and not a cluster-wide limitation

Jun Rao wrote a fantastic article titled How to choose the number of
topics/partitions in a Kafka cluster?

list of items to think about

Data correctness

The volume of messages of interest per consumer

How much data you will have or need to process

Data correctness is at the top of most data concerns in real-world designs

events that must be ordered

same partition and, thus, the same topic

it might be helpful to place the events with a message key

in two separate topics for the actual booked and confirmed/billed events

Next, we should consider the volume of messages

Most of the consumer’s time would be, in effect, filtering out the mass of
events to process only a select few

Another point to account for is the quantity of data

how the number of consumers in a group is limited by the partitions in our topic

it is important to know that partitions are not an unlimited free resource

last thing to consider when deciding on the number of partitions for a topic
is that reducing that number is not currently supported

removal of a partition could lose its current position

### 7.1.1 Topic-creation options

core options that must be set in order to create a topic

will ever need to delete a topic

Kafka requires us to enable the delete.topic.enable

kafka-topics.sh first to see what various actions

  bin/kafka-topics.sh

  bin/kafka-topics.sh --create

  bin/kafka-topics.sh --create --bootstrap-server localhost:9094 \
  --topic kinaction_topicandpart \ --partitions 2 \ --replication-factor 2

set auto.create.topics.enable to false

ensures that we create our

Listing 7.1 Listing our topic options

Listing 7.2 Listing our topic options with --create

Listing 7.3 Creating another topic

topics on purpose and not from a producer sending a message to a topic

producers and consumers do need to know the correct topic name of where
their data should live

we can indeed remove a topic if needed

  bin/kafka-topics.sh --delete --bootstrap-server localhost:9094 --topic kinaction_topicandpart

After running this command, you will not be able to work with
this topic for your data as before.

### 7.1.2 Replication factors

we should plan on having the total number of replicas less than or equal
to the number of brokers

InvalidReplicationFactorException

## 7.2 Partitions

From a consumer standpoint, each partition is an immutable log of messages

Listing 7.4 Deleting a topic

### 7.2.1 Partition location

look at how the data is stored on our brokers

log.dirs

log.dirs in your server.properties

.index, .log, and .timeindex

leader-epoch-checkpoint

.snapshot

files with the .log extension are where our data payload is stored

offset of the message as well as the CreateTime field

.index and .timeindex files to store a mapping between the logical message
offset and a physical position inside the index file

on a physical disk, a partition is not one single file but is rather split
into several segments

Older segments

governed for retention

can be eligible for topic compaction

### 7.2.2 Viewing our logs

Confluent has a script that we can use to look at those log segments

  bin/kafka-dump-log.sh --print-data-log \
  --files /tmp/kafkainaction/kafka-logs-0/ ➥ kinaction_topicandpart-1/\*.log \
  | awk -F: '{print $NF}' | grep kinaction

see how Kafka places messages on the broker and the data it retains around
those messages

Listing 7.5 Looking at a dump of a log segment

large number in the log filename

should be the same as the first offset

## 7.3 Testing with EmbeddedKafkaCluster

Kafka Streams provides an integration utility class called
EmbeddedKafkaCluster

in-memory Kafka cluster

Listing 7.6 Testing with EmbeddedKafkaCluster

### 7.3.1 Using Kafka Testcontainer

Testcontainers (https:// <www.testcontainers.org/modules/kafka/>

One of the coauthors of this book, Viktor Gamov, maintains a repository
(<https://github.com/gAmUssA/testcontainers-java-module-confluentplatform>)
of integration testing Confluent Platform components
(including Kafka, Schema Registry, ksqlDB).

## 7.4 Topic compaction

the goal is not to expire messages but rather to make sure that the
latest value for a key exists and not to maintain any previous state

configuration option

cleanup.policy=compact

  bin/kafka-topics.sh --create --bootstrap-server localhost:9094 \
  --topic kinaction_compact --partitions 3 --replication-factor 3 \
  --config cleanup.policy=compact

Listing 7.7 Creating a compacted topic

Summary

- Topics are non-concrete rather than physical structures.
To understand the topic’s behavior, a consumer of that topic needs
to know about the number of partitions and the replication factors in play.
- Partitions make up topics and are the basic unit for parallel processing
of data inside a topic.
- Log file segments are written in partition directories and are managed by
the broker.
- Testing can be used to help validate partition logic and may use an
in-memory cluster.
- Topic compaction is a way to provide a view of the latest value of a
specific record.
