# Anotações  
(14/04/2025, 17:07:36)

“Kafka storage”

“Another decision to consider is where our data should live long term”

“Kafka’s storage logically sits somewhere between the long-term storage solutions of a database and the transient storage of a message broker”

“This chapters covers  How long to retain data  Data movement into and out of Kafka  Data architectures Kafka enables  Storage for cloud instances and containers”

“8.1 How long to store data”

“default retention limit for data in Kafka topics is seven days”

“all of this data exists on a single broker drive”

“as partitions are not split between brokers.”

“How do we configure retention for brokers?”

“The main considerations are the size of the logs and the length of time the data exists.”

“disable log retention”

“etting both log.retention.bytes and log.retention.ms”

“imilar retention for the latest values by using keyed events”

“with a compacted topic”

“Another option for long-term storage is to move the”

“log.retention.bytes”

“he largest size threshold in bytes for deleting a log.”

“log.retention.ms”

“The length in milliseconds a log will be maintained before being deleted.”

“log.retention.minutes”

“Length before deletion in minutes. log.retention.ms is used as well if both are set.”

“log.retention.hours”

“Length before deletion in hours. log.retention.ms and log.retention.minutes would be used before this value if either of those are set.”

“data outside of Kafka and not retain it internally to the Kafka brokers themselves”

“we could store the data in a database, in a Hadoop Distributed File System (HDFSTM), or upload our event messages into something like cloud storage”

“8.2 Data movement”

“8.2.1 Keeping the original event”

“keep the original message”

“Having the original message makes it easier to go back and start over if you inadvertently messed up your transform logic”

“What if your models start to see trends on data that you once thought wouldn’t matter? By retaining all the data fields, you might be able to go back to that data and find insights you never expected”

“8.2.2 Moving away from a batch mindset”

“you can keep the pipeline running in nearreal time,”

“8.3 Tools”

“8.3.1 Apache Flume”

“Flume can provide an easier path for getting data into a cluster and relies more on configuration than on custom code”

“Listing 8.1 Flume configuration for watching a directory”

“Listing 8.2 Flume Kafka channel configuration”

“8.3.2 Red Hat® DebeziumTM”

“Debezium (https://debezium.io) describes itself as a distributed platform that helps turn databases into event streams.”

“deletes a user against the MySQL database instance that is being monitored for changes. Debezium captures the event written to the database’s internal log, and that event goes through the connector service and feeds into Kafka.”

“8.3.3 Secor”

“ims to help persist Kafka log data to a variety of storage options, including S3 and Google Cloud Storage”

“8.3.4 Example use case for data storage”

“Operational data is the events that are produced by our day-to-day operations.”

“8.4 Bringing data back into Kafka”

“load that data from S3 back into Kafka!”

“placing the data into a new Kafka topic. Our application can then run against Kafka, processing events without having to change any processing logic”

“8.4.1 Tiered storage”

“Confluent Platform version 6.0.0”

“emote storage is introduced for data that is older (and stored in a remote location) and controlled by time configuration (confluent.tier.local.hotset.ms)”

“8.5 Architectures with Kafka”

“the amount of data and the need to process that data in a timely manner were the drivers that led to some of these system designs”

“8.5.1 Lambda architecture”

“layers are discussed in Big Data by Marz”

“Batch”

“batch processing with MapReduce”

“system like Hadoop”

“Speed”

“except it produces views from recent data”

“Serving”

“views it sends to consumers”

“This real-time streaming layer is the most obvious place for Kafka to play a role”

“8.5.2 Kappa architecture”

“wanting to maintain a system that impacts your users without disruption”

“unning the current system in parallel to the new one”

“8.6 Multiple cluster setups”

“8.6.1 Scaling by adding clusters”

“Martin Fowler’s site at https:// martinfowler.com/bliki/CQRS.html”

“helps manage the performance of a large cluster by separating the load of producers sending data into Kafka from the sometimes much larger load of consumers reading the data”

“8.7 Cloud- and container-based storage options”

“KIP-392 shows an item that was accepted at the time of this writing, which seeks to help address the issues of a Kafka cluster spanning data centers. The KIP is titled “Allow consumers to fetch from the closest replica””

“8.7.1 Kubernetes clusters”

“Kafka applications will likely use the StatefulSet API in order to maintain the identity of each broker across failures or pod moves.”

“Confluent for Kubernetes helps as well with our Kubernetes managemen”

“Summary”

“ Data retention should be driven by business needs. Decisions to weigh include the cost of storage and the growth rate of our data over time.  Size and time are the basic parameters for defining how long data is retained on disk.  Long-term storage of data outside of Kafka is an option for data that might need to be retained for long periods. Data can be reintroduced as needed by producing the data into a cluster at a later time.  The ability of Kafka to handle data quickly and also replay data can enable architectures such as the lambda and kappa architectures.  Cloud and container workloads often involve short-lived broker instances. Data that needs to be persisted requires a plan for making sure newly created or recovered instances can utilize that data across all instances.”