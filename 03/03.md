
# Applying Kafka

Designing a Kafka project

*****

Designing a real-world Kafka project

- Determining which data format to use
- Existing issues impacting data usage
- Deciding when data transformation takes place
- How Kafka Connect helps us start a data-streaming path

*****

## 3.1 Designing a Kafka project

*****

one benefit of dealing with existing architectures is that it gives us a list
of pain points, that we can address

*****

helps us to highlight the shift in think-ing about the data in our work

*****

### 3.1.1 Taking over an existing data architecture

*****

Sensors are placed throughout the bike that continuously provide events about
the condition and status of the internal equipment they are monitoring

*****

### 3.1.2 A first change

*****

all our data does not have to move into Kafka at once

*****

take one database table and start our new architecture while letting the
existing applications run for the time being

*****

### 3.1.3 Built-in features

*****

purpose of Kafka Connect is to help move data into or out of Kafka without
writ-ing our own producers and consumers

*****

Connect can take a typical application log file and move it into a Kafka topic.

*****

connect-standalone.properties

- connect-file-source.properties

*****

take data from one data source and put that into Kafka so that we can treat
data as being sourced from a Kafka file

*****

With configurations (and not code), we can get data into Kafka from any file

*****

Listing 3.1 Configuring Connect for a file source

*****

We can start the Connect process by invoking the shell script

  connect-standalone.sh,

*****

Listing 3.2 Starting Connect for a file source Listing

*****

bin/connect-standalone.sh config/connect-standalone.properties \
alert-source.properties

*****

create a text file named alert.txt in the directory in which we started the
Connect service

*****

Connect should ingest this file’s contents and produce the data into Kafka [2].

*****

Listing 3.3 Confirming file messages made it into Kafka Listing

*****

  bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9094 \
  --topic kinaction_alert_connect --from-beginning

*****

sink connector and how it carries Kafka’s messages back out to another file

*****

Listing 3.4 Configuring Connect for a file source and a sink

*****

alert-sink.txt, and verify that you can see the messages that were in the
source file and that these were sent to the Kafka topic.

*****

### 3.1.4 Data for our invoices

*****

Connect standardizes interaction with other systems

*****

updates from a local database to a Kafka topic

*****

similar to the guide at
<https://docs.confluent.io/kafka-connect-jdbc/current/source-connector/index.html>,
if you need more reference material.

*****

Listing 3.7 Starting Connect for a database table source Creates

*****

confluent-hub install confluentinc/kafka-connect-jdbc:10.2.0 confluent local
services connect start ...

See Commands.md for other steps confluent local services

  connect connector config jdbc-source --config etc/kafka-connect-jdbc/kafkatest-sqlite.properties

*****

## 3.2 Sensor event design

*****

### 3.2.1 Existing issues

*****

two of those challenges: data silos and recoverability.
Currently, we have the sensors send their events for storage to a clustered
database solution. This is one of the main parts of our data design that we
want to change!
In the line, each gear represents a major step in our process.
Each step has a sensor attached.
We can dismiss this quality check in the line.
Critical path >– An admin console issues commands to the sensors.

*****

DEALING WITH DATA SILOS

*****

data and the processing are owned by an application

*****

The shift from traditional “data thinking” makes the data available to
everyone in its raw source

*****

RECOVERABILITY

*****

failure is an expected condition

*****

data retention makes it available for use again and again.

*****

application can replay those messages already con-sumed with the new
application logic.

*****

WHEN SHOULD DATA BE CHANGED?

*****

preference is to get the data into Kafka first

*****

### 3.2.2 Why Kafka is the right fit

*****

their current database is getting expensive to scale vertically

*****

With the ability to horizontally scale our cluster, we can hope to get
more overall benefits for our buck

*****

### 3.2.3 Thought starters on our design

*****

it is good to keep in mind when some of the features and APIs we might use
made their debut.

*****

this might be a good time to decide how we want our data to exist.

*****

think about how we want to process our data

*****

These preferences impact various parts of our design, but our main focus here
is on figuring out the data structure

*****

Is it okay to lose any messages in the system?

*****

Does your data need to be grouped in any way? Are the events correlated with
other events that are coming in?

*****

Do you need data delivered in a specific order? What if a message gets
delivered in an order other than when it occurred?

*****

Do you only want the last value of a specific item, or is the history of that
item important?
Do you care about how your data has evolved?

*****

How many consumers are you going to have? Will they all be independent of
each other, or will they need to maintain some sort of order when reading
the mes-sages?

*****

### 3.2.4 User data requirements

*****

ability to capture messages even if the consuming service is down

*****

want the status from our sensors as either working or broken (a sort of
alert), and we want to make sure we can see if any part of our bike process
could lead to total failure.

*****

maintain a history of the sensors’ alert status

*****

keep an audit log of any users that push updates or queries directly
against the sen-sors

*****

who did what administration actions on the sensors themselves

*****

### 3.2.5 High-level plan for applying our questions

*****

we should not lose messages, as our audit would not be com-plete without
all the event

*****

we do not need any grouping key because each event can be treated as
independent.

*****

order does not matter inside our audit topic because each message will have
a timestamp in the data itself

*****

the message payload can include time

*****

helpful to group this data using a key.

*****

Kafka adds the new event that it receives to the end of its log file like any
other message it receives. After all, the log is immutable and can only be
appended to at the end of the file

*****

### 3.2.6 Reviewing our blueprint

*****

Audit data

- Alert trend data
- Alert data

*****

write each event type from the sensors to their logical topic

*****

## 3.3 Format of your data

*****

Schemas are a means of providing some of this needed information in a way that
can be used by our code or by other applications that may need the same data.

*****

Avro provides schema definition sup-port as well as schema storage in Avro
files

*****

commonly used in Kafka.
Table 3.4 Audit checklist Kafka feature Concern?
Message loss No Grouping Yes Ordering No Last value only Yes Independent
consumer No

*****

### 3.3.1 Plan for data

*****

producers and consumers are not tied directly to each other

*****

Kafka does not do any data validation by default

*****

need for each process or application to understand what that data means

*****

By using a schema, we provide a way for our application’s developers to
understand the structure and intent of the data

*****

Avro schema defined as JSON

*****

Listing 3.8 Avro schema example

*****

Schemas can be used by tools like Apache Avro to handle data that evolves

*****

### 3.3.2 Dependency setup

*****

Avro always is serialized with its schema

*****

The old data uses the schema that existed as part of its data. On the other
hand, any new formats will use the schema present in their data. Clients

*****

Listing 3.9 Adding Avro to pom.xml Adds

*****

Avro files found in the source directory we list and to output the generated
code to the specified output directory

*****

Listing 3.11 Alert schema: kinaction_alert.avsc Sets the artifact ID needed in
our pom.xml as a plugin Configures the Maven phase Configures the Maven goal

*****

"Time alert generated as UTC milliseconds from epoch" }, { Listing 3.10 Adding
the Avro Maven plugin to pom.xml

*****

one thing to note is that "doc" is not a required part of the definition.
However, there is certainly value in adding details that will help future
producer or consumer developers understand what the data means.

*****

kafka-avro-serializer

*****

Listing 3.12 Adding the Kafka serializer to pom.xml

*****

io.confluent.kafka.serializers.KafkaAvroSerializer

*****

Listing 3.13 Producer using Avro serialization

*****

KafkaAvroDeserial-izer

*****

Listing 3.14 Consumer using Avro serialization

*****

there are various configuration-driven behaviors that we can use to help us
sat-isfy our specific requirements.
