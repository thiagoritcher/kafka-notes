# Anotações  
(15/04/2025, 10:58:19)

“Schema registry”

“think about how you view Kafka the more you utilize it”

“maturity levels”

“Martin Fowler provides a great explanation for this at https://martinfowler.com/ bliki/MaturityModel.html”

“This chapters covers  Developing a proposed Kafka maturity model  The value schemas can provide for your data as it changes  Reviewing Avro and data serialization  Compatibility rules for schema changes over time”

“11.1 A proposed Kafka maturity model”

“check out the Confluent white paper titled, “Five Stages to Streaming Platform Adoption,””

“11.1.1 Level 0”

“Kafka as an enterprise service bus (ESB)”

“the drive might be offloaded to work with a backend system that a user knows will not send a response right away”

“benefit of allowing us to decouple a system”

“11.1.2 Level 1”

“most data produced is now brought into Kafka”

“llows us to have an operational, real-time data flow and gives us the ability to feed data quickly into analytical systems”

“11.1.3 Level 2”

“schemas are needed”

“still need a way to understand the data itself”

“11.1.4 Level 3”

“Everything is an event stream that is infinite”

“we don’t have customers waiting for recommendations”

“or status reports that used to be produced by an overnight batch-processing run”

“11.2 The Schema Registry”

“how we can plan for data to change over time”

“Confluent Schema Registry stores our named schemas and allows us to maintain multiple versions”

“Producers and consumers are not tied together, but they still need a way to discover the schema involved in the data from all clients”

“11.2.1 Installing the Confluent Schema Registry”

“uses Kafka as its storage layer with the topic name _schemas”

“When thinking about production usage, the Schema Registry should be hosted on a server separate from our brokers”

“11.2.2 Registry configuration”

“defaults located in the etc/ schema-registry/schema-registry.properties”

“use ZooKeeper to help complete the election of the primary node”

“ou can also use a Kafka-based primary election (using the configuration kafkastore.bootstrap .servers)”

“listeners=http://localhost:8081 kafkastore.connection.url=localhost:2181 kafkastore.topic=_schemas debug=true”

“bin/schema-registry-start.sh \ ./etc/schema-registry/schema-registry.properties”

“11.3 Schema features”

“REST API”

“for storing and fetching schemas”

“client libraries”

“for retrieving and managing local schemas”

“11.3.1 REST API”

“resources: schemas, subjects, compatibility, and config”

““subjects” might need some explanation”

“Listing 11.1 Schema Registry configuration”

“Listing 11.2 Starting the Schema Registry”

“key and value are treated as different subjects”

“we can version and change our schemas independently”

“key and value are serialized separately”

“curl -X GET http://localhost:8081/config”

“add a Content-Type header”

“use application/ vnd.schemaregistry.v1+json”

“11.3.2 Client library”

“chapter 3, we created a schema for an Alert”

“value.serializer”

“use the KafkaAvroSerializer”

“kaProperties.put("key.serializer", ➥ "org.apache.kafka.common.serialization.LongSerializer"); kaProperties.put("value.serializer", ➥ "io.confluent.kafka.serializers.KafkaAvroSerializer"); kaProperties.put("schema.registry.url", ➥ "http://localhost:8081"); Producer<Long, Alert> producer = new KafkaProducer<Long, Alert>(kaProperties); Alert alert = new Alert(); alert.setSensorId(12345L); alert.setTime(Calendar.getInstance().getTimeInMillis()); alert.setStatus(alert_status.Critical);”

“Listing 11.3 Getting the Schema Registry configuration”

“Listing 11.4 Producer using Avro serialization”

“Because we use the default TopicNameStrategy”

“In this case, we could have used an override to use an underscore”

“to keep our topic name from having a mix of dashes and underscores”

“kaProperties.put("key.deserializer", ➥ "org.apache.kafka.common.serialization.LongDeserializer"); kaProperties.put("value.deserializer", ➥ "io.confluent.kafka.serializers.KafkaAvroDeserializer"); kaProperties.put("schema.registry.url", ➥ "http://localhost:8081");”

“only one version of a schema”

“Listing 11.5 Consumer using Avro deserialization”

“11.4 Compatibility rules”

“compatibility strategy we plan to support”

“those marked as transitive”

“types noted by Confluent: BACKWARD (the default type), BACKWARD_TRANSITIVE, FORWARD, FORWARD_TRANSITIVE, FULL, FULL_TRANSITIVE, and NONE”

“Backward-compatible changes might involve adding non-required fields or removing fields”

“we will likely want our consumer clients to upgrade first for the BACKWARD type”

“With the FORWARD type”

“we will likely want to update our producer clients first”

“{"name": "Alert", ... "fields": [ {"name": "sensor_id", "type": "long", "doc":"The unique id that identifies the sensor"}, ... {"name": "recovery_details", "type": "string", "default": "Analyst recovery needed"} ] }”

“Any older messages with version 1 of the schema will have a default value populated for the field added later.”

“Schema Registry version 2”

“11.4.1 Validating schema modifications”

“think about how we can automate testing changes to our schemas”

“Listing 11.6 Alert schema change”

“ Use the REST API compatibility resource endpoints  Use a Maven plugin for JVM-based applications”

“curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \ --data '{ "schema": "{ \"type\": \"record\", \"name\": \"Alert\", ➥ \"fields\": [{ \"name\": \"notafield\", \"type\": \"long\" } ]}" }' \ http://localhost:8081/compatibility/subjects/kinaction_schematest-value/ ➥ versions/latest {"is_compatible":false}”

“<plugin> <groupId>io.confluent</groupId> <artifactId> kafka-schema-registry-maven-plugin </artifactId> <configuration> <schemaRegistryUrls> <param>http://localhost:8081</param> </schemaRegistryUrls> <subjects> <kinaction_schematest-value> src/main/avro/alert_v2.avsc </kinaction_schematest-value> </subjects> <goals> <goal>test-compatibility</goal> </goals> </configuration> ... </plugin>”

“Listing 11.7 Checking compatibility with the Schema Registry REST API”

“Listing 11.8 Checking compatibility with the Schema Registry Maven plugin”

“11.5 Alternative to a schema registry”

“One such option is to produce data on a different topic with a breaking change”

“Summary  Kafka has many features that you can use for simple use cases or all the way up to being the major system of an enterprise.  Schemas help version our data changes.  The Schema Registry, a Confluent offering apart from Kafka, provides a way to work with Kafka-related schemas.  As schemas change, compatibility rules help users know whether the changes are backward, forward, or fully compatible.  If schemas are not an option, different topics can be used to handle different versions of data.”