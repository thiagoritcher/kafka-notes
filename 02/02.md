
# Getting to know Kafka

*****

 The high-level architecture of Kafka  Understanding client options  How applications communicate with a broker  Producing and consuming your first message  Using Kafka clients with a Java application

*****

distributed system at heart, but it is also possible to install and run it on a single host.

*****

appendix A if you do not have a Kafka cluster to use or are inter-ested in starting one locally on your machine

*****

Producing and consuming a message

*****

message, also called a record,

*****

the basic piece of data flowing through Kafka
Kafka.in.Action.Dylan.Scott.Manning.9781617295232.EBooksWorld.ir

Custom headers can be used if desired as well

*****

Keys and values will be the focus of most of our discussion in this chapter,

*****

Each key and value can interact in its own specific ways to serialize or deserialize its data

*****

You will deliver this message to Kafka by sending it to what are known as brokers.

*****

2.2 What are brokers?

*****

server side of Kafka

*****

will have three Kafka serv-ers running for most of our examples

*****

To create a topic, we will run the kafka-topics.sh command in a shell window

*****

with the --create option

*****

s: ~/kafka_2.13-2.7.1/bin

*****

Listing 2.1 Creating the kinaction_helloworld topic

*****

bin/kafka-topics.sh --create --bootstrap-server localhost:9094 --topic kinaction_helloworld --partitions 3 --replication-factor 3

*****

We can avoid many frustrating errors and warnings by not including spaces or various special characters

*****

The --partitions option determines how many parts we want the topic to be split int

*****

more than one partition at this stage lets us see how the system works in spreading data across partitions.

*****

The --replication-factor also is set to three in this example. In essence, this says that for each partition, we want to have three replicas. These

*****

The --bootstrap-server option points to our local Kafka broker. This is why the broker should be running before invoking this script

*****

Listing 2.2 Verifying the topic

*****

bin/kafka-topics.sh --list --bootstrap-server localhost:9094 To

*****

bin/kafka-topics.sh --bootstrap-server localhost:9094 \ --describe --topic kinaction_helloworld

*****

When we reference a partition leader in the image, we are referring to a replica leader. It is important to know that a partition can consist of one or more replicas, but only one replica will be a leader. A leader’s role involves being updated by external clients, whereas nonleaders take updates only from their leader.

*****

There is a configuration to enable or disable the autocreation of topics. However, it is usually best to control the creation of topics as a specific action

*****

Listing 2.4 Kafka producer console command Listing

*****

bin/kafka-console-producer.sh --bootstrap-server localhost:9094 \ --topic kinaction_helloworld

*****

Listing 2.5 Kafka consumer command Listing

*****

bin/kafka-console-consumer.sh --bootstrap-server localhost:9094 \ --topic kinaction_helloworld --from-beginning

*****

As we send more messages and confirm the delivery to the consumer application, we can terminate the process and eliminate the --from-beginning option when we restart it. Notice that we didn’t see all of the previously sent messages. Only those messages produced since the consumer console was started show up

*****

2.3 Tour of Kafka

*****

Table 2.1 The Kafka architecture Component Role Producer Sends messages to Kafka Consumer Retrieves messages from Kafka Topics Logical name of where messages are stored in the broker ZooKeeper ensemble Helps maintain consensus in the cluster Broker Handles the commit log (how messages are stored on the disk) Producer

*****

2.3.1 Producers and consumers

*****

A producer is a tool for sending messages to Kafka topics

*****

Listing 2.7 A producer sending messages

*****

In contrast to a producer, a consumer is a tool for retrieving messages from Kafka

*****

Consuming applications subscribe to the topics that they are interested in and continuously poll for data.

*****

calls a subscribe method,

*****

passing in a list of topics that it wants to gather data from

*****

2.3.2 Topics overview

*****

Topics consist of units called partitions [1]

*****

partitions are what Kafka works with for the most part

*****

we talk about brokers in later chapters. Although segment files make up partitions, you will likely not interact directly with them, and this should be considered an internal implementation detail.

*****

every partition will have an

*****

elected leader replica.

*****

But how does your producer or consumer know which partition replica is the leader? In the event of distributed computing and random failures, that answer is often influ-enced with help from ZooKeeper

*****

2.3.3 ZooKeeper usage

*****

Apache ZooKeeper (http:/ /zookeeper.apache.org/) is a dis-tributed store that provides discovery, configuration, and synchronization services in a highly available way.

*****

brokers need to not only communicate with each other, they also need to reach an agreement.

*****

Agreeing on which one is the replica leader of a partition is one example of the practical application of ZooKeeper within the Kafka ecosystem

*****

2.3.4 Kafka’s high-level architecture

*****

core Kafka can be thought of as Scala application processes that run on a Java virtual machine (JVM)

*****

what is it about Kafka’s design that makes this possible

*****

usage of the operating system’s page cache

*****

access pattern of data

*****

new messages flood in, it is likely that the latest messages are of more interest to many consumers, which can then be served from this cache

*****

Kafka uses its own protocol

*****

reworked the message format to compress messages more effectively

*****

The protocol could change and be specific to the needs of the creators of Kafka.

*****

2.3.5 The commit log

*****

The log used in Kafka is not just a detail that is hidden

*****

It is front and center, and its users employ offsets to know where they are in that log

*****

events are always added to the end.

*****

The persistence as a log itself for storage is a major part of what separates Kafka from other message brokers. Reading a message does not remove it from the system or exclude it from other consumers

*****

how long can I retain data in Kafka?

*****

In var-ious companies today, it is not rare to see that after the data in Kafka commit logs hits a configurable size or time retention period

*****

data is often moved into a perma-nent store.

*****

Kafka is made to keep its performance fast even while keeping its messages

*****

2.4 Various source code packages and what they do

*****

2.4.1 Kafka Streams

*****

One of the sweet spots for Kafka Streams is that no separate processing cluster is needed. It is meant to be a lightweight library to use in your application

*****

However, it still has powerful features, including local state with fault tolerance, one-at-a-time message processing, and exactly-once support

*****

This API was made to ensure that creating streaming applications is as easy as pos-sible, and it provides a fluent API, similar to Java 8’s Stream API

*****

Microservice designs are also being influenced by this API. Instead of data being isolated in various applications, it is pulled into applications that can use data independently

*****

2.4.2 Kafka Connect

*****

created to make integrations with other systems easier

*****

Source connectors are used to import data from a source into Kafka

*****

sink connectors are used to export data from Kafka into different systems

*****

Kafka Connect is an excellent choice for making quick and sim-ple data pipelines that tie together common systems

*****

2.4.3 AdminClient package

*****

This interface is a great tool that will come in handy the more involved we become with Kafka’s administration

*****

2.4.4 ksqlDB

*****

allowed developers and data analysts who used mostly SQL for data analysis to leverage streams by using the interface they have known for years

*****

The mindset shift to a continuous query over a data stream is a significant shift and a new viewpoint for developers

*****

2.5 Confluent clients

*****

Confluent provides a matrix of supported features by programming language at the following site to help you out:
https:/ /docs.confluent.io/ current/clients/index.html. As

*****

The code in listing 2.9 is a simple producer

*****

The key.serializer and value.serializer parameters are also something to take note of in development. We need to provide a class that will serialize the data as it moves into Kafka. Keys and values do not have to use the same serializer

*****

One of the most important sections to note in listing 2.11 is the poll call on the con-sumer. This is what is actively trying to bring messages to our application

*****

2.6 Stream processing and terminology

*****

depend on data coming into and out of its core to provide value to its users.
Producers send data into Kafka, which works as a distributed system for reliability and scale, with logs, which are the basis for storage. Once

*****

2.6.1 Stream processing

*****

2.6.2 What exactly-once means

*****

however, we will touch on what these semantics mean for Kafka’s everyday usage.

*****

Various Kafka Connect connectors also support exactly-once and are great examples of bringing data out of Kafka

--
Lendo livros com ReadEra
https://play.google.com/store/apps/details?id=org.readera&hl=pt
