
Brokers

*****

ker Although we have focused on the client side of Kafka so far, our focus will now shift to another powerful component of the ecosystem: brokers. Broke

6.1 Introducing the broker

*****

Kafka has a rack awareness feature that makes replicas for a partition exist physically on sep-arate racks [1].

*****

6.2 Role of ZooKeeper

*****

ZooKeeper needs to have a minimum number in order to elect leaders and reach a decision, this cluster is indeed important for our brokers [3]. ZooKeeper itself holds information such as topics in our cluster [4]. ZooKeeper helps the brokers by coordi-nating assignments and notifications

*****

Certain legacy frame-works we use might also provide a means of connecting our client application with our ZooKeeper cluster.

*****

zkNodes property

*****

Using the Kafka tool zookeeper-shell.sh,

*****

paths like /controller, /controller _epoch, /config, and /brokers,

*****

ZkData.scala

*****

different Kafka tool, kafka-topics.sh,

*****

Listing 6.1 Listing our topics Connects

*****

bin/zookeeper-shell.sh localhost:2181

*****

bin/kafka-topics.sh --list \ ➥--bootstrap-server localhost:9094

*****

Even when ZooKeeper no longer helps to power Kafka, we might need to work with clusters that have not migrated

*****

Clients will be writing to and read-ing from brokers to get information into and out of Kafka, and they will demand bro-ker attention [9].

*****

6.3 Options at the broker level

*****

server.properties file

*****

This file is a common way to pass a specific configuration to a broker instance.

*****

also deals with configurations related to listeners, log locations, log retention, ZooKeeper, and group coordinator settings

*****

If we terminate the broker with ID 2 in this example and then try to consume a mes-sage for that topic, we would get a message such as “1 partitions have leader brokers without a matching listener.”

*****

6.3.1 Kafka’s other logs: Application logs

*****

Kafka base installation directory under the folder logs/. We can change this loca-tion by editing the config/log4j.properties file and the value for kafka.logs.

*****

6.3.2 Server log

*****

server.log, is where we would look if there is a startup error or

*****

they are located on each broker. They are not aggregated by default into one location.

*****

6.3.3 Managing state

*****

each partition has a single leader replica. A leader replica resides on a single broker at any given time. A broker can host the leader replica of multiple partitions, and any broker in a cluster can host leader replicas.

*****

Only one broker in the cluster, however, acts as the controller.

*****

Listing 6.3 Listing the current controller Connects to your ZooKeeper instance Uses get against the

*****

bin/zookeeper-shell.sh localhost:2181 get /controller

*****

6.4 Partition replica leaders and their role

*****

One of the replicas of the partition will have the job of being the leader

*****

The leader is in charge of handling writes from external producer clients for that partition.

*****

Replicas act as consumers of the leader partition and will fetch the messages

*****

Because broker 3 is not available, a new leader is elected.

*****

In-sync replicas (ISRs) are a key piece to really understanding Kafka.

*****

you lose a broker on which one of your copies of a partition exists, Kafka does not (currently) create a new copy

*****

An important item to look at when monitoring the health of our systems is how many of our ISRs are indeed matching our desired number.

*****

important to note that if a replica starts to get too far behind in copying messages from the leader, it can be removed from the ISR list.

*****

6.4.1 Losing data

*****

if we have no ISRs and lose our lead replica due to a failure?

*****

When unclean .leader.election.enable is true, the controller selects a leader for a partition even if it is not up to date so that the system keeps running

*****

The problem with this is that data could be lost because none of the replicas have all the data at the time of the leader’s failure.

*****

At the cost of missing data, this option allows us to keep serving clients.

*****

6.5 Peeking into Kafka

*****

Grafana® (https:/ /grafana.com/) and Prometheus® (https:/ /prometheus .io/) as examples of tools that can be used to help set up a simple monitoring stack that can be use

*****

The Kafka exporter takes the JMX notifications and exports them into the Prometheus format.

*****

For the Kafka exporter, an outstanding option is available at https:/ /github.com/ danielqsj/kafka_exporter.

*****

To get more JMX metrics, we can set the JMX_PORT environment variable when starting our Kafka pro-cesses

*****

Listing 6.4 Starting a broker with a JMX port

*****

JMX_PORT=$JMX_PORT bin/kafka-server-start.sh \ ➥config/server0.properties

*****

6.5.1 Cluster maintenance

*****

various pieces of the ecosystem such as Kafka and Connect clients, Schema Registry, and the REST Proxy do not usually run on the same servers

*****

for safety and efficiency, we definitely don’t want all of these processes running on a single server when we handle production work-loads.

*****

6.5.2 Adding a broker

*****

just start a new Kafka bro-ker with a unique ID.

*****

can either be created with the configuration broker.id or with broker.id.generation.enable set to true [10].

*****

new broker will not be assigned to any partitions

*****

6.5.3 Upgrading your cluster

*****

One technique that can be used to avoid downtime for our Kafka applications is the rolling restart

*****

upgrading one broker at a time.

*****

An important broker configuration property for rolling restarts is controlled.shut-down.enable. Setting this to true enables the transfer of partition leadership before a broker shuts down

*****

6.5.4 Upgrading your clients

*****

Clients can usually be upgraded after all of the Kafka brokers in a cluster are upgraded.

*****

take a peek at the version notes to make sure newer versions are compatible.

*****

6.5.5 Backups

*****

Rather than performing manual copies and coordinating across brokers, one preferred option is for a cluster to be backed by a second cluster

*****

kafka-mirror-maker

*****

Confluent Replicator

*****

Cluster Link-ing

*****

6.6 A note on stateful systems

*****

Confluent’s site on using the Kubernetes Confluent Operator API (https:/ /www.confluent.io/confluent-operator/

*****

Strimzi™ (https:/ /github.com/ strimzi/strimzi-kafka-operator), if you are looking at running your cluster on Kuberne-tes.

*****

Gwen Sha-pira explores further in her paper, “Recommendations for Deploying Apache Kafka on Kubernetes” [26].

*****

can be helpful is to run Kafka clients and appli-cations on a Kubernetes cluster before the Kafka brokers.

*****

6.7 Exercise

*****

Listing 6.5 Describing the topic replica: a test for ISR count

*****

--under-replicated-parti-tions flag to see any problems quickly [27].

*****

Listing 6.6 Using the under-replicated-partitions flag

*****

bin/kafka-topics.sh --describe --bootstrap-server localhost:9094 \ --under-replicated-partitions

*****

TIP When using any of the commands in this chapter, it is always a good idea to run the command without any parameters and read the command options that are available for troubleshooting.

--
Lendo livros com ReadEra
https://readera.org
