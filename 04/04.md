# Producers: Sourcing data

*****

its importance, is only one part of this system. In fact, we can change some
producer configuration options or set these at the broker or topic level.

*****

## 4.1 An example

*****

could be the IoT events we are using in our fictional company

*****

Currently, the user submits a form on the website that generates email to a
support account or chat

*****

We want to keep this information coming to us but in a way that allows the
data to be more accessible than in an email inbox

*****

Notice that the application has an HTML form but writes to a

*****

Kafka topic, not to an email server

*****

and it can be used in many ways

*****

### 4.1.1 Producer notes

*****

producer’s job includes fetching metadata about the cluster

*****

produc-ers can only write to the replica leader of the partition they are
assigned to

*****

the meta-data helps the producer determine which broker to write to

*****

Because this distributed system is designed to account for momentary errors
such as a network blip, the logic for retries is already built in

*****

if the ordering of the messages is essential,

*****

retries to a number like 3, we also need to set the
max.in.flight.requests.per.connection value to 1 and set acks
(the number of brokers that send acknowledgments back)

*****

to all [3

*****

## 4.2 Producer options

*****

producer interceptors

*****

main goal was to help sup-port measuring and monitoring

*****

ease of setting options using the Java clients

*****

Using the value from the property bootstrap.servers as a start-ing point,
the producer fetches metadata about brokers and partitions that it uses for
all subsequent writes.

*****

Kafka allows you to change key behaviors just by changing some configuration
values.

*****

ProducerConfig

*****

Importance label of “high” in the Confluent website

*****

most crucial producer configuration

*****

Table 4.1 Important producer configurations

*****

Key Purpose acks Number of replica acknowledgments that a producer requires
before success is established bootstrap.servers One or more Kafka brokers to
connect for startup value.serializer The class that’s used
for serialization of the value key.serializer The class that’s used for
serialization of the key 4.2.1

*****

Key Purpose acks Number of replica acknowledgments that a producer requires
before success is established bootstrap.servers One or more Kafka brokers to
connect for startup value.serializer The class that’s used
for serialization of the value key.serializer The class that’s used for
serialization of the key

*****

### 4.2.1 Configuring the broker list

*****

we have to tell the pro-ducer which topic to send messages to.

*****

We, however, do not have to know the details of those partitions when we send
messages.

*****

Producer clients can also overcome a failure of the partition leader they are
writing to by using the information about the cluster to find a new leader.

*****

### 4.2.2 How to go fast (or go safer)

*****

the acks key, which stands for acknowledgments. This controls how many
acknowledgments the producer needs to receive from the partition leader’s
followers before it returns a completed request.

*****

values for this property are all, -1, 1, and 0 [9].

*****

Setting this value to 0 will probably get us the lowest latency but at the
cost of safety.

*****

values all or -1 are the strongest available

*****

value all means that a partition leader’s replica waits on the entire list
of its in-sync replicas (ISRs) to acknowledge completion

*****

setting the acks value to 1 and asking for an acknowledgment.

*****

This is closely related to the ideas of at-most and at-least semantics that
we covered in chapter 1 [10]. The acks setting is a part of that larger
picture.

*****

### 4.2.3 Timestamps

*****

producer record contain a timestamp on the events you send.

*****

it can be a broker timestamp that occurs when the message is logged.

*****

message.timestamp.type

*****

CreateTime uses the time set by the client, whereas setting it to
LogAppendTime uses the broker time [11].

*****

reading timestamped data is often thought of as a con-sumer client concern,

*****

If a retry happens and other requests succeed on their first attempt, earlier
messages might be added after the later ones.

*****

## 4.3 Generating code for our requirements

*****

One requirement was that there was no need to correlate (or group together)
any events

*****

sting 4.2 Waiting for a result

*****

Waiting on the response directly in a synchronous way ensures that the code
is han-dling each record’s results as they come back before another message
is sent.

*****

But what if we have a specific format we want to produce?

*****

We’ll use seri-alization to translate data into a format that can be
transmitted

*****

Listing 4.3 Alert class

*****

If you see or hear the term serde, it means that the serializer and
deseri-alizer are both handled by the same implementation of that
interface [13].

*****

Some sort of agreement or coordinator is needed for the data formats for
clients even though Kafka does not care what data it stores on the brokers.

*****

We will keep an eye on the distribution of the size of the partitions to note
if they become uneven in the future,

*****

We do not care about the history of the status checks, only the current
scenario. In this case, we also want to partition our alerts.

The default for a message with no key (which we used in the examples thus far)
was a round-robin assignment strategy prior to Kafka version 2.4.

*****

In versions after 2.4, messages without keys use a sticky partition strategy
[14]. However, some-times we have specific ways that we want our data to be
partitioned. One way to take control of this is to write our own unique
partitioner class.

*****

The client also has the ability to control what partition it writes to by
configuring a unique partitioner.

*****

Let’s say we have four levels of alerts:
Critical, Major, Minor, and Warning. We could create a partitioner that places
the dif-ferent levels in different partitions.

*****

critical alerts are directed to a specific partition (like partition 0).

*****

Listing 4.6 Partitioner for alert levels

*****

implementing the Partitioner interface, we can use the partition method to
send back the specific partition we want our producer to write to

*****

we need to set the configura-tion key, partitioner.class, for our producer to
use the specific class we created.

*****

Listing 4.8 shows the configuration of the producer to add the
partitioner.class value to use as our specific partitioner.

*****

Listing 4.8 Alert producer

*****

in listing 4.8 is how we added a callback to run on completion.

*****

Although we said that we are not 100% concerned with message failures from
time to time, due to the frequency of events, we want to make sure that we do
not see a high failure rate that could be a hint at application-related errors.

*****

example of implementing a Callback interface.

*****

Listing 4.9 Alert callback

*****

public void onCompletion (RecordMetadata metadata, Exception exception)

*****

Flume version 1.8

*****

The following listing shows a configuration snippet that would be used by
a Flume agent.

*****

Listing 4.10 Flume sink configuration

*****

The KafkaSink source code from Apache Flume, found at http:/ /mng.bz/JvpZ,
provides an example of taking data and placing it inside Kafka with producer code.

*****

Listing 4.11 Reading the Kafka producer configuration from a file

*****

Client and broker versions

*****

One important thing to note is that Kafka broker and client versions do not
always have to match.
